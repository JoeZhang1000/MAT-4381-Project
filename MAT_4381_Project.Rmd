---
title: "Comparing the Bayesian Linear Regression Method to Ordinary Least Squares"
author: "Joe Zhang and Christian Paravalos"
date: "Winter 2024"
output: 
  pdf_document:
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base")
```

```{=latex}
\newpage
```

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
\newpage
```

# Abstract

Linear regression is often used to model relationships between a response variable and one or more explanatory variables. In traditional settings, the ordinary least squares method is used to estimate the model. In this paper, our goal is to develop a linear model by estimating the model parameters under a Bayesian framework and compare the model against the traditional ordinary least squares model using a dataset.

# Introduction

To model the relationship between a response variable and one or more explanatory variables, the simplest way to estimate a model is to assume that there is a linear relationship between the response variable and the predictor variables. The formulation of the model is $$\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon} \text{, }$$ where $X$ is an $n \times k$ matrix with the observations of the explanatory variables, $\boldsymbol{y}$ is an $n \times 1$ vector of observations of the response variables, $\boldsymbol{\beta}$ is a $k \times 1$ vector of model parameters, and $\boldsymbol{\epsilon}$ is an $n \times 1$ vector of random errors associated with each observation, $n$ is the number of observations, and $k$ is the number of predictors (plus the intercept term). The goal is to estimate $\boldsymbol{\beta}$ and then construct and evaluate the model from the estimated values. In other words, we have

$$\boldsymbol{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \text{, } \quad X = \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1(k-1)} \\ 1 & x_{21} & x_{22} & \cdots & x_{2(k-1)} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & x_{n2} & \cdots & x_{n(k-1)} \end{bmatrix} \text{, } \quad \boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{k-1} \end{bmatrix} \text{, } \quad \boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$$

The most well-known way is to use the ordinary least squares method to estimate the parameters. The goal of this paper is to compare the ordinary least squares method with the Bayesian method of estimating parameters.

# Ordinary Least Squares Regression Revisited

## Estimation of Model Parameters

Consider a multiple linear regression model $$\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon} \text{, }$$ assuming that the random errors have mean 0, variance $\sigma^2$, are uncorrelated, and follow a normal distribution. In other words, we assume that $\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 I)$, where $\boldsymbol{0}$ is the zero vector, and $I$ is the identity matrix.

To estimate the coefficients, the following quantity should be minimized: $$S(\boldsymbol{\beta}) = \sum_{i = 1}^{n} \epsilon_i^2 = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon} = (\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta})$$

We need to take derivatives of $S(\boldsymbol{\beta})$ with respect to $\boldsymbol{\beta}$. We get that $$\frac{\partial S(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = -2X^T\boldsymbol{y} + 2X^TX\boldsymbol{\beta}$$

If we set this equation to 0 and we solve for $\boldsymbol{\beta}$, we get $$X^TX\boldsymbol{\beta} = X^T\boldsymbol{y}$$

If we multiply both sides by the inverse of $X^TX$, assuming that it is invertible, then the least squares estimator of $\boldsymbol{\beta}$ is $$\hat{\boldsymbol{\beta}} = (X^TX)^{-1}X^T\boldsymbol{y}.$$

The fitted model is as follows:

$$\hat{\boldsymbol{y}} = X\hat{\boldsymbol{\beta}}$$

The least squares estimator of $\boldsymbol{\beta}$ has the following properties:

```{=tex}
\begin{itemize}
  \item $\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$
  \item $Var[\hat{\boldsymbol{\beta}}] = \sigma^2 (X^TX)^{-1}$
\end{itemize}
```
## Estimation of Variance of Error Terms

In the frequentist approach, we estimate $\sigma^2$ using the residual sum of squares. We have $$SS_{res} = \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} e_i^2 = \boldsymbol{e}^T\boldsymbol{e}$$ where $\hat{y_i}$ are the fitted values of $y_i$ using the least squares model and $e_i = y_i - \hat{y}_i$ are the residuals. $\boldsymbol{e}$ is the vector of the residuals. If we simplify further, we get $$SS_{res} = \boldsymbol{y}^T\boldsymbol{y} - \hat{\boldsymbol{\beta}}^TX^T\boldsymbol{y}$$

The residual sum of squares has $n-k$ degrees of freedom. The residual mean square is $$MS_{res} = \frac{SS_{res}}{n-k}$$

We can show that $$\mathbb{E}[MS_{res}] = \sigma^2$$

Therefore, an unbiased estimator for $\sigma^2$ is given by $$\hat{\sigma^2} = MS_{res} = \frac{\boldsymbol{y}^T\boldsymbol{y} - \hat{\boldsymbol{\beta}}^TX^T\boldsymbol{y}}{n-k}$$

## Inference on Model Parameters

### t-tests

After obtaining the estimates of the model parameters, we need to make inference on them. The main goal of inference is to determine if all the explanatory variables are significant to the model. Sometimes when there are too many variables, the accuracy of predictions using the model can decrease significantly. Making inferences can help decide if some variables should not be included in the final model.

We have assumed that $\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 I)$. This consequently leads to that $\boldsymbol{y}|X \sim N_n(X\boldsymbol{\beta}, \sigma^2I)$. Under this assumption, we can also get that $$\hat{\boldsymbol{\beta}} \sim N_k(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1})$$

From this, we can also deduce that $$\hat{\beta_j} \sim N(\beta_j, \sigma^2 (X^TX)^{-1}_{jj})$$ for $j = 1, 2, ..., k$ and $(X^TX)^{-1}_{jj}$ is the j-th diagonal element of the matrix $(X^TX)^{-1}$.

If we standardize $\hat{\beta}$, we get that $$\frac{\hat{\beta_j} - \beta_j}{\sigma \sqrt{(X^TX)^{-1}_{jj}}} \sim N(0, 1)$$

In practice, we do not know the value of $\sigma^2$, which we will need to use in order to make inference on the model parameters. Therefore, the best way is to use the estimate for $\sigma^2$, which is $MS_{res}$. However, if we replace $\sigma^2$ with $MS_{res}$, we get $$\frac{\hat{\beta_j} - \beta_j}{\sqrt{MS_{res}(X^TX)^{-1}_{jj}}} \sim t(n - k)$$

If we want to test the null hypothesis $H_0: \beta_j = \beta_{j_0}$ against the hypothesis $H_1: \beta_j \neq \beta_{j_0}$, we have that under $H_0$ that $$t_0 = \frac{\hat{\beta_j} - \beta_{j_0}}{\sqrt{MS_{res}(X^TX)^{-1}_{jj}}} \sim t(n - k)$$

$t_0$ is the test statistic for the hypothesis $\beta_j = \beta_{j_0}$. We reject $H_0$ if $|t_0| > t_{\alpha/2}(n-k)$, where $\alpha$ is the significance level and $t_{\alpha/2}(n-k)$ is the $100(1 - \alpha/2)$th quantile of the Student t distribution with $n - k$ degrees of freedom. We can also compute the p-value, which is $$p\text{-value} = 2P(T > |t_0|)$$and we reject $H_0$ if the p-value is less than $\alpha$.

Another way to test the hypothesis $H_0$ against $H_1$ is to construct confidence intervals for the model parameters $\beta_j$. We can deduce that a $100(1 - \alpha)\%$ confidence interval for $\beta_j$ is $$\hat{\beta_j} \pm t_{\alpha/2}(n-k) \cdot \sqrt{MS_{res} (X^TX)^{-1}_{jj}}$$

From the confidence interval, we accept $H_0$ if $\beta_{j_0}$ is contained in the interval. Otherwise, we reject $H_0$.

Note that the above explanation of OLS regression was based on Montgomery, Peck, and Vining (2021, pp. 12-91).

# Bayesian Linear Regression

Now, we turn to the Bayesian approach to estimate a linear regression model between a response variable and several explanatory variables. The form of the model is the same as in the ordinary least squares framework $$\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

## Estimation of Parameters

We make the same assumptions as in the ordinary least squares framework, where $\boldsymbol{\epsilon} \sim N_n(\boldsymbol{0}, \sigma^2 I)$. We need to estimate $k + 1$ parameters: the $k$ model coefficients, and $\sigma^2$.

### Likelihood Function

With the assumption, we have that $$\boldsymbol{y}|X, \boldsymbol{\beta}, \sigma^2 \sim N_n(X\boldsymbol{\beta}, \sigma^2I)$$

Therefore, the likelihood function is $$f(\boldsymbol{y}| X, \boldsymbol{\beta}, \sigma^2) = \frac{1}{(2\pi)^{n/2}(\sigma^2)^{n/2}} \exp \left[-\frac{1}{2\sigma^2}(\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) \right]$$

### Choosing a prior distribution

To estimate $\boldsymbol{\beta}$ and $\sigma^2$, we treat them as random variables and we make a prior assumption on their distribution. The assumptions we will use are $$\boldsymbol{\beta}|\sigma^2 \sim N_k(\boldsymbol{m}, \sigma^2V)$$ and $$\sigma^2 \sim IG(a, b)$$ where $\boldsymbol{m}$ is a $k \times 1$ vector of real numbers, $V$ is a $k \times k$ matrix of real numbers, and $a$ and $b$ are both real numbers.

Then, we get that the prior for $(\boldsymbol{\beta}, \sigma^2)$ is $$g(\boldsymbol{\beta}, \sigma^2) = g(\boldsymbol{\beta} | \sigma^2)g(\sigma^2)$$ where $$g(\boldsymbol{\beta}|\sigma^2) = \frac{1}{(2\pi \sigma^2)^{k/2}}|V|^{-\frac{1}{2}} \exp \left(-\frac{1}{2\sigma^2} (\boldsymbol{\beta} - \boldsymbol{m})^TV^{-1}(\boldsymbol{\beta} - \boldsymbol{m}) \right)$$ and $$g(\sigma^2) = \frac{b^a}{\Gamma(a)} (\sigma^2)^{-(a+1)} \exp \left( -\frac{b}{\sigma^2} \right)$$ for $\sigma^2 > 0$.

### Computation of posterior distribution

Now that we have the likelihood function and the prior probability distribution of the parameters, we can now compute the posterior distribution of the parameters. We will follow a similar procedure to Wundervald (2019). We let $h(\boldsymbol{\beta}, \sigma^2 | \boldsymbol{y}, X)$ represent the posterior distribution function:

```{=tex}
\begin{equation*}
  \begin{split}
    h(\boldsymbol{\beta}, \sigma^2 | \boldsymbol{y}, X)
    & \propto f(\boldsymbol{y}| X, \boldsymbol{\beta}, \sigma^2) g(\boldsymbol{\beta}, \sigma^2) \\
    & \propto f(\boldsymbol{y}| X, \boldsymbol{\beta}, \sigma^2) g(\boldsymbol{\beta} | \sigma^2) g(\sigma^2) \\
    & \propto (2 \pi \sigma^2)^{-\frac{n}{2}} \exp \left( -\frac{1}{2\sigma^2} (\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) \right) (2 \pi \sigma^2)^{-\frac{k}{2}} |V|^{-\frac{1}{2}} \exp \left( -\frac{1}{2\sigma^2} (\boldsymbol{\beta} - \boldsymbol{m})^TV^{-1}(\boldsymbol{\beta} - \boldsymbol{m}) \right) \\
    & \times \frac{b^a}{\Gamma(a)} (\sigma^2)^{-(a+1)} \exp \left(-\frac{b}{\sigma^2} \right) \\
    & \propto (\sigma^2)^{-\frac{n}{2} - \frac{k}{2} - (a + 1)} \exp \left( -\frac{1}{2 \sigma^2} [(\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) + (\boldsymbol{\beta} - \boldsymbol{m})^TV^{-1}(\boldsymbol{\beta} - \boldsymbol{m}) + 2b] \right) \\
    & \propto (\sigma^2)^{-\frac{n}{2} - \frac{k}{2} - (a + 1)} \exp \left( -\frac{1}{2 \sigma^2} A \right)
  \end{split}
\end{equation*}
```
We now want to rewrite the equation above by simplifying the quantity inside the exponential function. We do the following:

```{=tex}
\begin{equation*}
  \begin{split}
    A
    &= (\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) + (\boldsymbol{\beta} - \boldsymbol{m})^TV^{-1}(\boldsymbol{\beta} - \boldsymbol{m}) + 2b \\
    &= \boldsymbol{y}^T\boldsymbol{y} - \boldsymbol{y}^TX\boldsymbol{\beta} - \boldsymbol{\beta}^TX^T \boldsymbol{y} + \boldsymbol{\beta}^TX^TX\boldsymbol{\beta} + \boldsymbol{\beta}^TV^{-1}\boldsymbol{\beta} - \boldsymbol{\beta}^TV^{-1}\boldsymbol{m} - \boldsymbol{m}^TV^{-1}\boldsymbol{\beta} + \boldsymbol{m}^TV^{-1}\boldsymbol{m} + 2b \\
    &= \boldsymbol{\beta}^T(X^TX+V^{-1})\boldsymbol{\beta} - \boldsymbol{\beta}^T(X^T\boldsymbol{y} + V^{-1}\boldsymbol{m}) + (\boldsymbol{m}^TV^{-1}\boldsymbol{m} + 2b + \boldsymbol{y}^T\boldsymbol{y}) - (\boldsymbol{y}^TX + \boldsymbol{m}^TV^{-1})\boldsymbol{\beta}
  \end{split}
\end{equation*}
```
We define the following quantities:

$$\Lambda = (X^TX + V^{-1})^{-1}$$ and $$\boldsymbol{\mu} = (X^TX + V^{-1})^{-1}(X^T\boldsymbol{y} + V^{-1}\boldsymbol{m})$$

where $\Lambda$ is a $k \times k$ matrix and $\boldsymbol{\mu}$ is a $k \times 1$ vector.

Then we can write

```{=tex}
\begin{equation*}
  \begin{split}
    A
    &= \boldsymbol{\beta}^T\Lambda^{-1}\boldsymbol{\beta} - \boldsymbol{\beta}^T\Lambda^{-1}\boldsymbol{\mu} - \boldsymbol{\mu}^T\Lambda^{-1}\boldsymbol{\beta} + \boldsymbol{m}^TV^{-1}\boldsymbol{m} + 2b + \boldsymbol{y}^T\boldsymbol{y} \\
    &= (\boldsymbol{\beta} - \boldsymbol{\mu})^T \Lambda^{-1} (\boldsymbol{\beta} - \boldsymbol{\mu}) - \boldsymbol{\mu}^T \Lambda^{-1} \boldsymbol{\mu} + \boldsymbol{m}^TV^{-1}\boldsymbol{m} + 2b + \boldsymbol{y}^T\boldsymbol{y}
  \end{split}
\end{equation*}
```
Now, we go back to the posterior distribution and we can write:

```{=tex}
\begin{equation*}
  \begin{split}
    h(\boldsymbol{\beta}, \sigma^2 | \boldsymbol{y}, X)
    & \propto (\sigma^2)^{-\frac{n}{2} - \frac{k}{2} - (a + 1)} \exp \left( -\frac{1}{2 \sigma^2} A \right) \\
    & \propto (\sigma^2)^{-\frac{n}{2} - \frac{k}{2} - (a + 1)} \exp \left( -\frac{1}{2 \sigma^2} ((\boldsymbol{\beta} - \boldsymbol{\mu})^T \Lambda^{-1} (\boldsymbol{\beta} - \boldsymbol{\mu}) - \boldsymbol{\mu}^T \Lambda^{-1} \boldsymbol{\mu} + \boldsymbol{m}^TV^{-1}\boldsymbol{m} + 2b + \boldsymbol{y}^T\boldsymbol{y}) \right) \\
    & \propto (\sigma^2)^{-\frac{k}{2}} \exp \left(-\frac{(\boldsymbol{\beta} - \boldsymbol{\mu})^T \Lambda^{-1} (\boldsymbol{\beta} - \boldsymbol{\mu})}{2 \sigma^2} \right) (\sigma^2)^{-(\frac{n}{2} + a + 1)} \exp \left(- \frac{\boldsymbol{m}^TV^{-1}\boldsymbol{m} - \boldsymbol{\mu}^T \Lambda^{-1} \boldsymbol{\mu} + 2b + \boldsymbol{y}^T\boldsymbol{y}}{2\sigma^2}\right)
  \end{split}
\end{equation*}
```
From looking at the posterior distribution, we can see that it is the product of a multivariate normal distribution and an inverse gamma distribution. We can parameterize the posterior the following way:

$$h(\boldsymbol{\beta}, \sigma^2| \boldsymbol{y}, X) = h(\boldsymbol{\beta} | \sigma^2, \boldsymbol{y}, X)h(\sigma^2|\boldsymbol{y}, X)$$ where $$h(\boldsymbol{\beta}| \sigma^2, \boldsymbol{y}, X) = \frac{1}{(2 \pi \sigma^2)^{\frac{k}{2}}}|\Lambda|^{-\frac{1}{2}} \exp \left( -\frac{(\boldsymbol{\beta} - \boldsymbol{\mu})^T \Lambda^{-1} (\boldsymbol{\beta} - \boldsymbol{\mu})}{2\sigma^2}\right)$$ and $$h(\sigma^2| \boldsymbol{y}, X) = \frac{(\frac{1}{2}(\boldsymbol{m}^TV^{-1}\boldsymbol{m} - \boldsymbol{\mu}^T\Lambda^{-1}\boldsymbol{\mu} + 2b + \boldsymbol{y}^T\boldsymbol{y}))^{\frac{n}{2} + a}}{\Gamma(a + \frac{n}{2})} (\sigma^2)^{-(\frac{n}{2} + a + 1)} \exp \left( -\frac{\boldsymbol{m}^TV^{-1}\boldsymbol{m} - \boldsymbol{\mu}^T\Lambda^{-1}\boldsymbol{\mu} + 2b + \boldsymbol{y}^T\boldsymbol{y}}{2 \sigma^2}\right)$$ for $\sigma^2 > 0$. Then we get that $$\boldsymbol{\beta} | \sigma^2, \boldsymbol{y}, X \sim N_k(\boldsymbol{\mu}, \sigma^2 \Lambda)$$ and $$\sigma^2| \boldsymbol{y}, X \sim IG(\alpha, r)$$ where $$\boldsymbol{\mu} = (X^TX + V^{-1})^{-1}(X^T\boldsymbol{y} + V^{-1} \boldsymbol{m})$$ $$\Lambda = (X^TX + V^{-1})^{-1}$$ $$\alpha = a + \frac{n}{2}$$ $$r = \frac{\boldsymbol{m}^TV^{-1}\boldsymbol{m} - \boldsymbol{\mu}^T\Lambda^{-1}\boldsymbol{\mu} + 2b + \boldsymbol{y}^T\boldsymbol{y}}{2}$$

## Making Bayesian inference

We now have the posterior distribution of $(\boldsymbol{\beta}, \sigma^2)$. From that, we also get that $$\boldsymbol{\beta} | \sigma^2, \boldsymbol{y}, X \sim N_k(\boldsymbol{\mu}, \sigma^2 \Lambda)$$ and $$\sigma^2 | \boldsymbol{y}, X \sim IG(\alpha, r)$$

The main goal is to estimate the parameters. We first need to find the marginal distribution of $\boldsymbol{\beta}$. This is hard to do by hand since we have to solve the following integral: $$h(\boldsymbol{\beta} | \boldsymbol{y}, X) = \int_{0}^{\infty} h(\boldsymbol{\beta} | \sigma^2, \boldsymbol{y}, X) h(\sigma^2| \boldsymbol{y}, X) \, d(\sigma^2)$$ Therefore, we need to use computational methods to simulate the posterior marginal distribution for $\boldsymbol{\beta}$.

To make inference on individual parameters, we have that since $\boldsymbol{\beta}$ follows a multivariate normal distribution, so we can also conclude that for any model parameter $\beta_j$, we get that $$\beta_j | \sigma^2, \boldsymbol{y}, X \sim N(\mu_j, \sigma^2  \Lambda_{jj})$$ where $\mu_j$ is the j-th element of the vector $\boldsymbol{\mu}$ and $\Lambda_{jj}$ is the j-th diagonal element of the matrix $\Lambda$. Then the marginal distribution for $\beta_j$ is $$h(\beta_j| \boldsymbol{y}, X) = \int_{0}^{\infty} h(\beta_j | \sigma^2, \boldsymbol{y}, X) h(\sigma^2| \boldsymbol{y}, X)\, d(\sigma^2)$$ This integral is also hard to compute, so we need to use computational methods.

Given the marginal distribution, we can use it to find the mean of $\beta_j$ (best estimate under squared error loss) and we can also construct credible regions to decide if the explanatory variable associated with the parameter is significant to the model. A $100(1 - \alpha)\%$ credible region for a parameter $\beta_j$ is the region $I$ such that $P(\beta_j \in I | \text{Data}) = 1 - \alpha$. There are many different such regions, but our goal is to find the shortest credible region since we want a more accurate estimate. If the shortest $95\%$ credible region contains 0, then we can conclude that the parameter and its associated explanatory variable are not significant to the model. Otherwise, we conclude that they are significant.

## Simulation Algorithm to Compute Posterior

To simulate the marginal distribution for a parameter $\beta_j$, we can use the following algorithm:

1.  Load the data.

2.  Initialize the chosen parameters $\boldsymbol{m}$, $V$, $a$, and $b$.

3.  Compute $\boldsymbol{\mu}$, $\Lambda$, $\alpha$, and $r$ using the data and the initialized prior parameters.

4.  Simulate $\sigma^2$ a large number of times from the $IG(\alpha, r)$ distribution.

5.  For each simulated value of $\sigma^2$, simulate $\beta_j$ from the $N(\mu_j, \sigma^2 \Lambda_{jj})$ distribution.

6.  Draw a graph for the simulated distribution of $\beta_j$, then compute its mean and the shortest 95% credible region.

# Application

Now we have established the theory behind Bayesian linear regression, we will compare the Bayesian linear regression model to the ordinary least squares model on the Boston Housing dataset. The data was derived from information collected by the U.S. Census Service concerning housing in the area of Boston, Massachusetts (Harrison & Rubinfeld, 1978). Note that it is believed that the median house prices were censored past 50 (\$50,000) (any median home price recorded above \$50,000 was set to \$50,000).

## Data Description and Preprocessing

The Boston Housing dataset contains information about various factors affecting housing prices in the Boston area. It includes features such as the per capita crime rate, average number of rooms per dwelling, proportion of residential land zoned for lots over 25,000 square feet, and more.

The variables of the data are as follows:

```{=tex}
\begin{itemize}
  \item CRIM: per capita crime rate by town
  \item ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
  \item INDUS: proportion of non-retail business acres per town
  \item CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)   
  \item NOX/NX: nitric oxides concentration (parts per 10 million)
  \item RM: average number of rooms per dwelling
  \item AGE: proportion of owner-occupied units built prior to 1940
  \item DIS: weighted distances to five Boston employment centres
  \item RAD: index of accessibility to radial highways
  \item TAX: full-value property-tax rate per \$10,000
  \item PTRATIO: pupil-teacher ratio by town
  \item B: $1000(Bk - 0.63)^2$ where Bk is the proportion of [people of African American descent] by town
  \item LSTAT: \% lower status of the population
  \item MEDV: Median value of owner-occupied homes in \$1000s (target variable)
\end{itemize}
```
Our goal is to construct a linear regression model that describes the relationship between the median value of owner-occupied homes and the factors that affect housing prices in the Boston area. We build a model using both the frequentist and the Bayesian framework. We are removing the variable CHAS because it is an indicator variable. We construct the models and then we compare them.

To make things easier to read, we will be using the following notations for variables:

```{=tex}
\begin{itemize}
  \item $X_1: CRIM$
  \item $X_2: ZN$
  \item $X_3: INDUS$
  \item $X_4: NOX/NX$
  \item $X_5: RM$
  \item $X_6: AGE$
  \item $X_7: DIS$
  \item $X_8: RAD$
  \item $X_9: TAX$
  \item $X_{10}: PRATIO$
  \item $X_{11}: B$
  \item $X_{12}: LSTAT$
  \item $Y: MEDV$
\end{itemize}
```
We first divide the data into training and test sets randomly:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(caret)
library(olsrr)
```

```{r}
#Set seed for reproducibility
set.seed(1)

boston_housing_dataset <- read.csv("Boston.csv")

#Remove the indicator variable for river 
boston_housing_dataset <- subset(boston_housing_dataset, select = -CHAS)

#Create train and test sets
train_set_indices <- createDataPartition(boston_housing_dataset$MEDV, p = 0.6, list = FALSE)

train_set_dataset <- boston_housing_dataset[train_set_indices, ]

test_set_dataset <- boston_housing_dataset[-train_set_indices, ]

#For use in future Python code
write.csv(train_set_dataset, "train_set_dataset.csv", row.names = FALSE)
write.csv(test_set_dataset, "test_set_dataset.csv", row.names = FALSE)
```

```{r}

#Draw plot of MEDV
ggplot(train_set_dataset, aes(x = MEDV)) +
  geom_histogram(bins = 20, fill = "#7b79d4", color = "#0e0d42") +
  labs(title = "Histogram of Median Value of Owner-Occupied Homes in $1000s (MEDV)", 
       x = "Median Value of Owner-Occupied Homes (in $1000s)", y = "Frequency") + 
  theme_minimal() 

```

```{=latex}

Figure 1: Histogram of median value of owner-occupied homes in \$1000s (response/target variable of regression)
```
As seen in Figure 1, the response variable MEDV is somewhat normally distributed (the data does seem to be more concentrated to the lower half of the histogram), with a build up of data points around 50. This is a result of the median house prices being censored past \$50,000 in the dataset.

## Ordinary Least Squares Model

We first fit a linear regression model under the frequentist framework. Then we make inference and evaluate the quality of the model. We consider 2 possible models: one using all predictors and one that is selected using stepwise selection. (Note we wanted to explain the $R^2$, ANOVA/F-tests between full and reduced models, significance of regression, and analysis of the residuals, but as we have not developed the theory for similar methods in the Bayesian setting, we will not go through them in this paper to be consistent).

We first fit the full model with all variables:

```{r}

full_model=lm(MEDV ~ .,data=train_set_dataset)

summary(full_model)

```

We have that the full frequentist linear regression model fitted using the training dataset is $\hat{Y} = 32.044 - 0.122 X_1 + 0.043 X_2 + 0.067 X_3 - 13.995 X_4 + 4.022 X_5 - 0.005 X_6 - 1.470 X_7 + 0.343 X_8 - 0.014 X_9 - 0.856 X_{10} + 0.012 X_{11} - 0.630 X_{12}$, where the variables are as described in the previous section. From looking at the summary table of the model, we see that the variables $X_3$ (INDUS) and $X_6$ (AGE) are not statistically significant under the significance level $\alpha = 0.05$ because the p-values from the t-tests are greater than 0.05.\

We now compute training and test MSE:

```{r}

RSS <- residuals(full_model)^2

training_MSE_full <- mean(RSS)

cat("Training MSE:", training_MSE_full, "\n")

cat("Training MSE (adjusted for #parameters):", 
    training_MSE_full*nrow(train_set_dataset)/
      (nrow(train_set_dataset) - length(coef(full_model))), "\n")

test_MSE <- mean((test_set_dataset$MEDV - predict(full_model, newdata=test_set_dataset))^2)

cat("Test MSE:", test_MSE)

```

```{r, echo=FALSE}
full_model_summary <- summary(full_model)$coefficients

write.csv(data.frame(
  betaEstimate = full_model_summary[, "Estimate"],
  standardError = full_model_summary[, "Std. Error"],
  lower95CIBound = confint(full_model, level = 0.95)[, "2.5 %"],
  upper95CIBound = confint(full_model, level = 0.95)[, "97.5 %"]), "full_model_summary.csv", row.names = TRUE)

```

We will evaluate the model using the training and test MSE (Mean Square Error). The training error is 21.666 and the test error is 24.705. Note that the MSE used does not account for the number of parameters in the model as we wanted to be consistent between the training and test MSE computations, where in the test setting the number of parameters is not as relevant (want lowest test error independent of #parameters). If we accounted for the number of parameters in the model, we would get a training MSE of 22.627.

Now, we build a reduced linear regression model using stepwise selection (allow predictors to enter the model with t-test p-values less than 10% and be removed with t-test p-values greater than 15% (p-value/t-test is computed based on the model developed at the current step of the stepwise process)):

```{r}

stepwise_model_results <- ols_step_both_p(full_model, details = FALSE, p_enter = 0.10, p_remove = 0.15)

stepwise_model <- lm(MEDV ~ CRIM + ZN + NX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT, 
                     data = train_set_dataset)

summary(stepwise_model)

```

We have that the reduced model selected via stepwise regression is $\hat{Y} = 31.665 - 0.123X_1 + 0.043X_2 - 13.172X_4 + 3.952X_5 -1.508X_7 + 0.325 X_8 - 0.013 X_9 - 0.824 X_{10} + 0.011 X_{11} - 0.626X_{12}$. All predictors are statistically significant in this model based on the t-tests, indicating that we removed the non-significant predictors from the full model that were not significantly explaining the median home prices.

```{r}

RSS <- residuals(stepwise_model)^2

training_MSE_reduced <- mean(RSS)

cat("Training MSE:", training_MSE_reduced, "\n")

cat("Training MSE (adjusted for #parameters):", 
    training_MSE_reduced*nrow(train_set_dataset)/
    (nrow(train_set_dataset) - length(coef(stepwise_model))), "\n")

test_MSE <- mean((test_set_dataset$MEDV - predict(stepwise_model, newdata=test_set_dataset))^2)

cat("Test MSE:", test_MSE)
```

```{r, echo=FALSE}
reduced_model_summary <- summary(stepwise_model)$coefficients

write.csv(data.frame(
  betaEstimate = reduced_model_summary[, "Estimate"],
  standardError = reduced_model_summary[, "Std. Error"],
  lower95CIBound = confint(stepwise_model, level = 0.95)[, "2.5 %"],
  upper95CIBound = confint(stepwise_model, level = 0.95)[, "97.5 %"]), "stepwise_model_summary.csv", row.names = TRUE)
```

After evaluating the model, we obtain a training MSE of 21.730 and test MSE of 24.527. The training MSE that accounts for the number of parameters in the model is around 22.541, which is slightly lower than the one for the full model (which was 22.627). This indicates that the reduced model, when penalizing for the increase in parameters in the full model, explains the model slightly better than the full model (captures less noise). Note that the test MSE is also slightly lower with this model, which is a further confirmation that removing the non-significant predictors $X_3$ and $X_6$ from the model did not help explain median home prices (removing them is beneficial to decrease the complexity of the model). Remember that the median house prices are given in \$1000s, so the test MSE signify that the mean square error on the test dataset for the full and reduced models are around \$24,705 and \$24,527, respectively. Note we will talk about estimates for the variance of the error terms in a future section.

## Bayesian Linear Regression Model

### Setting Prior Distribution Parameters

We must now choose the prior parameters for each of the prior distributions.

For setting the parameters for the joint prior for the $\beta$ coefficients (given $\sigma^2$), we will set $\boldsymbol{m}=0$ (mean of each beta coefficient is set to 0) and $V$ to $25I$. Now, when we set the means to 0 and $V^{-1}$ to $\lambda I$ (in other words $V=\frac{1}{\lambda}I$), where $I$ is the identity matrix and $\lambda \geq 0$, we can see that the posterior best estimates under squared error loss for the $\beta$ parameters are the same as in the frequentist method of ridge regression (Montgomery, Peck, & Vining, 2021, pp. 312-315). The larger $\lambda$ is, the more shrinking of estimates occur toward 0, with the goal of making the beta coefficient estimates of non-significant predictors close to 0. The purpose of this is to attempt to not overfit the training data while being able to more easily generalize to unseen data by minimizing the influence of unexplanatory predictors (want to capture less noise that is mistaken for signal in the model). We set $\lambda=\frac{1}{25}$ to balance the potential for overfitting versus underfitting the model (too large a $\lambda$ value could lead to underfitting).

For the prior for $\sigma^2$, we used the $IG(a, b)$ prior with $a = 5$ and $b = 75$. $\sigma^2$ is the variance of the random error terms. Error comes up from what we miss from the model we choose, such as if the true relationship is not linear, if there are measurement errors, or if there are other lurking variables that cause variation in the response variable (James, Witten, Hastie, & Tibshirani, 2013, p. 63). We picked a prior that tries to minimize the variance of the random error because we believe that our data source is high quality (from U.S. Census Service), that our predictors capture the majority of key factors influencing median home prices, and that the true relationship between the predictors and median home prices is approximately linear. As seen below, the prior distribution chosen has a mode of 12.5, mean of 18.75 (both relatively low), and variance of 36.5 indicating that we are not overconfident in our prior. We ensured that the distribution tails off quickly past around 50, which is the maximum value of the median home prices in the population that this dataset was sampled from (since any price above \$50,000 was censored to \$50,000).

```{python, echo=FALSE}
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from tabulate import tabulate
import random
```

```{python}

#Set the random seed to make the code reproducible
random.seed(1)

#Load the training and test data that has been preprocessed
train_set = pd.read_csv('train_set_dataset.csv', header=0, sep=',')
test_set = pd.read_csv('test_set_dataset.csv', header=0, sep=',')

num_predictors = len(train_set.columns)

#Set the parameters for the prior distribution for sigma2 (inverse gamma) (non-informative)
a = 5
b = 75

#Set the parameters for the prior distribution for mu (normal) (non-informative)
V = np.diag([25] * num_predictors)
m = np.zeros(num_predictors)

#Plot of prior for sigma^2 and generate statistics for it
sigma2 = 1 / np.random.gamma(shape=a, scale=1/b, size=1000000)
mode = b/(a+1)
mean = b/(a-1)
variance = (b^2)/((a-1)^2 *(a-2))
print(f"Statistics for sigma^2 prior \n Mode: {mode}, Mean: {mean}, Variance: {variance}")

plt.hist(sigma2, bins=1000, alpha=1, color='green', range=(0, 100))
plt.title('Histogram of Simulated Distribution of Prior for $\\sigma^2$ ($IG(5, 75)$)')
plt.xlabel('$\\sigma^2$ values')
plt.ylabel('Frequency')
plt.show()
```

```{=latex}
Figure 2: Histogram of simulated distribution of prior for $\sigma^2$
\newpage
```
### Algorithm

We now execute the algorithm for finding the posterior distribution for $\sigma^2$ and the marginal posterior distributions for $\beta_i, i \in \{1, \dots, 12\}$, with the $i^{th}$ $\beta$ coefficient ($\beta_i$) corresponding to the $i^{th}$ predictor ($X_i$) ($\beta_0$ is intercept term).

```{python}

#Some preprocessing
train_set.insert(0, 'Intercept', 1)
test_set.insert(0, 'Intercept', 1)

X_training_set = train_set.iloc[:, 0:num_predictors].to_numpy()
y_training_set = train_set.iloc[:, num_predictors].to_numpy()

X_test_set = test_set.iloc[:, 0:num_predictors].to_numpy()
y_test_set = test_set.iloc[:, num_predictors].to_numpy()

#Set sampling size
sampling_size = 3000000

#Compute the inverse of V
V_inv = np.linalg.inv(V)

#Compute the mean vector of the posterior distribution of beta given sigma2 (normal)
mu_post = (np.linalg.inv(X_training_set.T @ X_training_set + V_inv) 
           @ (X_training_set.T @ y_training_set + V_inv @ m))

#Compute the lambda matrix of the posterior distribution of beta given sigma2 (normal)
lambda_post = np.linalg.inv(X_training_set.T @ X_training_set + V_inv)

#Compute alpha and r paramater of sigma2 posterior distribution (IG(alpha, r))
alpha_post = a + len(y_training_set) / 2
r_post = (m.T @ V_inv @ m - mu_post.T @ np.linalg.inv(lambda_post) 
          @ mu_post + 2*b + y_training_set.T @ y_training_set) / 2

# Draw sigma2 from its posterior inverse gamma distribution
sigma2 = 1 / np.random.gamma(shape=alpha_post, scale=1/r_post, size=sampling_size)

beta_hat = np.zeros((sampling_size, num_predictors))

#Sample the joint posterior distributions for the Beta coefficients
for i in range(sampling_size):
    beta_hat[i, :] = np.random.multivariate_normal(mu_post, lambda_post * sigma2[i])

```

### Plots

```{python}

colors = LinearSegmentedColormap.from_list("gradient", ["red", "purple", "blue"], N=num_predictors)

beta_estimates = pd.DataFrame(index=train_set.columns[0:num_predictors], 
                              columns=["Best Posterior Beta Estimate (under Squared Error Loss)", 
                              "95% Credible Region"])

fig, axes = plt.subplots(5, 3, figsize=(35, 46))
axes = axes.flatten()

#Create the graphs
for i in range(num_predictors):
    sorted_beta_hat_i = np.sort(beta_hat[:, i])
    value_at_975 = round(sorted_beta_hat_i[int(np.floor(0.975 * sampling_size))], 8)
    value_at_25 = round(sorted_beta_hat_i[int(np.floor(0.025 * sampling_size))], 8)

    beta_estimates.iloc[i, 0] = mu_post[i]
    beta_estimates.iloc[i, 1] = f"[{value_at_25}, {value_at_975}]"

    subaxis = axes[i]
    n, bins, patches = subaxis.hist(beta_hat[:, i], 
                                    bins=int(sampling_size / 1000), color=colors(i), alpha=1)
    low_cr = subaxis.axvline(x=value_at_25, color='r', 
                             linestyle='--', linewidth=5, 
                             label = 'Lower 95% CR Bound for $\\beta_i$ Posterior')
    high_cr = subaxis.axvline(x=value_at_975, color='darkred', 
                              linestyle='--',linewidth=5, 
                              label = 'Upper 95% CR Bound for $\\beta_i$ Posterior')
    mean_est = subaxis.axvline(x=mu_post[i], color='c', 
                               linestyle='--', linewidth=5, 
                               label = '$\\beta_i$ Posterior Best Estimate (under Squared Error Loss)')

    subaxis.set_xlabel('$\\beta_{' + str(i) + '}$ Value', fontsize=30)
    subaxis.set_ylabel('Frequency', fontsize=30)
    if (i == 0):
      subaxis.set_title(train_set.columns[i] + ' Histogram', fontsize=30)
    else:
      subaxis.set_title('$X_{' + str(i) + '}$ Histogram (' + train_set.columns[i] + ')', fontsize=30)
    subaxis.tick_params(axis='both', which='major', labelsize=20)

lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes][1:2]
lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]
fig.legend(lines, labels, loc='upper center', ncol=3, fontsize=30, bbox_to_anchor=(0.5, 0.94))

title = 'Histograms of Simulated Marginal Posterior Distributions for Beta Coefficients of Predictors'
fig.suptitle(title,
             fontsize=50, y=0.98)

plt.tight_layout(rect=[0, 0, 1, 0.92])

for i in range(num_predictors, 15):
    axes[i].axis('off')

plt.show()

```

```{=latex}
Figure 3: Histograms of simulated marginal posterior distributions for beta coefficients of predictors (with labels for shortest 95\% credible regions and mean of posterior distributions (best estimate for each beta coefficient under squared error loss))
```
```{python}
#Plot for posterior distribution for sigma^2
plt.hist(sigma2, bins=int(sampling_size/1000), alpha=1, color='orange')
plt.xlabel('$\\sigma^2$ values')
plt.ylabel('Frequency')
plt.title('Histogram of Simulated Posterior Distribution for $\\sigma^2$ ' + f'(IG({int(alpha_post)}, {int(r_post)}))')
plt.legend()
plt.show()

```

```{=latex}
Figure 4: Histogram of simulated posterior distribution for $\sigma^2$
\newpage
```
### Results

```{python, echo=FALSE, warning=FALSE, message=FALSE}
bayesian_results = tabulate(beta_estimates, headers='keys', tablefmt='latex',  
                            numalign="left", stralign="left")

with open('bayesian_results.tex', 'w') as f:
    f.write(bayesian_results)
    
f.close()
```

```{=latex}
\input{bayesian_results.tex}
\\
\\
Table 1: Summary of the Bayesian estimates for the model parameters, containing the Bayesian estimates under the squared error loss (mean/mode/median of posterior) and the shortest 95\% credible region.
```
We can see in Table 1 that our estimated Bayesian linear regression model is $\hat{Y} = 29.203 - 0.120X_1 + 0.043X_2 + 0.063X_3 - 12.474 X_4 + 4.164 X_5 - 0.005X_6 - 1.430 X_7 + 0.333 X_8 - 0.014 X_9 - 0.807 X_{10} + 0.012 X_{11} - 0.624 X_{12}$. Note that the best posterior estimates for each beta coefficient under the squared error loss is the same as the MAP (Maximum a Posteriori) estimate which equals the mode of the posterior distribution (since these are the same as the normal distribution is symmetric).

From the shortest 95% credible regions of the parameters, we can see that for the predictors $X_3$ (INDUS) and $X_6$ (AGE), their credible regions contain 0. From the hypothesis testing point of view, we are testing the null hypothesis that the beta coefficient for the predictor equals 0 against the alternative that it does not equal 0. 0 being contained in the 95% credible regions for the beta coefficients of $X_3, X_6$ indicates that these predictors are not statistically significant to explaining the median house prices in Boston towns (we accept the null hypotheses that the beta coefficients equal 0). We can also see that their best posterior estimates are relatively small. The shortest 95% credible regions for the other parameters don't contain 0, which means that they are statistically significant to the model (we reject the null hypotheses that the beta coefficients equal 0). 

```{python}
#Compute training and test MSE
training_MSE = np.mean((y_training_set - X_training_set @ mu_post)**2)

print(f"Training Set Bayesian MSE: {training_MSE}")

#Accounts for #parameters
training_MSE = np.mean((y_training_set - X_training_set @ mu_post)**2)*(len(y_training_set))/(len(y_training_set)-
                X_training_set.shape[1])

print(f"Training Set Bayesian MSE (accounting for #parameters): {training_MSE}")

test_MSE = np.mean((y_test_set - X_test_set @ mu_post)**2)

print(f"Test Set Bayesian MSE: {test_MSE}")
```

We can see that the training MSE of our Bayesian model on a training set is around 21.679 (around 22.641 adjusted for #parameters), while the test MSE is higher at around 24.866. As the training and test MSE are similar, there does not seem to be too much overfitting occurring.

Note that we will talk about the estimates for $\sigma^2$ in the next section, but briefly we mention that the posterior distribution for $\sigma^2$ is IG$(158, 3412)$.

## Comparison Between OLS and Bayesian Models

We will now compare the two methods based on their estimates for the $\beta$ coefficients and $\sigma^2$:

```{python, echo=FALSE, warning=FALSE, message=FALSE}
OLS_beta_estimates = pd.read_csv('full_model_summary.csv', index_col=0).rename(index={'(Intercept)': 'Intercept'})

beta_estimates_comparison = beta_estimates.rename(columns={'Best Posterior Beta Estimate (under Squared Error Loss)': 'Bayesian Beta Estimate'}).drop('95% Credible Region', axis=1)

beta_estimates_comparison['Beta Estimate Difference'] = beta_estimates['Best Posterior Beta Estimate (under Squared Error Loss)'] - OLS_beta_estimates['betaEstimate']
beta_estimates_comparison.insert(0, 'Full OLS Model Beta Estimate', OLS_beta_estimates['betaEstimate'])

beta_estimates_table = tabulate(beta_estimates_comparison, headers='keys', tablefmt='latex',  numalign="left", stralign="left")

with open('beta_estimates.tex', 'w') as f:
    f.write(beta_estimates_table)

```

```{python, echo=FALSE, warning=FALSE, message=FALSE}
ROLS_beta_estimates = pd.read_csv('stepwise_model_summary.csv', index_col=0).rename(index={'(Intercept)': 'Intercept'})

beta_estimates_comparison = beta_estimates.rename(columns={'Best Posterior Beta Estimate (under Squared Error Loss)': 'Bayesian Beta Estimate'}).drop('95% Credible Region', axis=1).drop(["AGE", "INDUS"], axis=0)

beta_estimates_comparison['Beta Estimate Difference'] = beta_estimates['Best Posterior Beta Estimate (under Squared Error Loss)'].drop(["AGE", "INDUS"], axis=0) - ROLS_beta_estimates['betaEstimate']
beta_estimates_comparison.insert(0, 'Reduced OLS Model Beta Estimate', ROLS_beta_estimates['betaEstimate'])

beta_estimates_table = tabulate(beta_estimates_comparison, headers='keys', tablefmt='latex',  numalign="left", stralign="left")

with open('reduced_beta_estimates.tex', 'w') as f: 
    f.write(beta_estimates_table)

```

```{python, echo=FALSE, warning=FALSE, message=FALSE}

beta_extra_info = pd.DataFrame()

OLS_beta_estimates['lower95CIBound'] = OLS_beta_estimates['lower95CIBound'].round(6)
OLS_beta_estimates['upper95CIBound'] = OLS_beta_estimates['upper95CIBound'].round(6)
beta_extra_info['Full OLS Model 95% Beta CI'] = '[' + OLS_beta_estimates['lower95CIBound'].astype(str) + ', ' + OLS_beta_estimates['upper95CIBound'].astype(str) + ']'
ROLS_beta_estimates['lower95CIBound'] = ROLS_beta_estimates['lower95CIBound'].round(6)
ROLS_beta_estimates['upper95CIBound'] = ROLS_beta_estimates['upper95CIBound'].round(6)
beta_extra_info['Reduced OLS Model 95% Beta CI'] = '[' + ROLS_beta_estimates['lower95CIBound'].astype(str) + ', ' + ROLS_beta_estimates['upper95CIBound'].astype(str) + ']'

beta_extra_info['Reduced OLS Model 95% Beta CI'][3] = ''
beta_extra_info['Reduced OLS Model 95% Beta CI'][6] = ''

beta_extra_info['Bayesian 95% Beta CR'] = beta_estimates['95% Credible Region']
beta_95_CI_CR_table = tabulate(beta_extra_info, headers='keys', tablefmt='latex',  numalign="left", stralign="left")

with open('beta_95_CI_CR_table.tex', 'w') as f:
    f.write(beta_95_CI_CR_table)
    
```

```{=latex}
\input{beta_estimates.tex}
\\
Table 2: Comparison of the ordinary least squares estimates (full model) and the Bayesian estimates of the beta coefficients minimizing squared error loss. Difference is computed as OLS - Bayesian
```
As seen in Table 2, we can see that the Bayesian estimates for the beta coefficients are not much different compared to the ordinary least squares estimates in the full OLS model. The last column of Table 2 shows the difference between the Bayesian and ordinary least squares estimates, using the following formula: OLS - Bayesian. The magnitude of the differences are almost all not very high (most are less than 1). This shows that the estimated linear regression models under both the ordinary least squares and the Bayesian framework are very similar. This is due to choosing a small $\lambda$ value in the Bayesian framework (the smaller $\lambda$ is chosen the less penalty is applied to large beta coefficients), resulting in a minor shrinkage in the beta coefficient estimates in the Bayesian setting. This small shrinkage in the Bayesian beta coefficients estimates can be seen in the table. It is evident that for the majority of beta coefficients, our best estimates under the Bayesian setting shrunk slightly towards 0 compared to the least squares estimates under the full model.

```{=latex}
\input{reduced_beta_estimates.tex}
\\
Table 3: Comparison of the ordinary least squares estimates (reduced model) and the Bayesian estimates of the beta coefficients minimizing squared error loss. Difference is computed as OLS - Bayesian
```
Similarly to the full model, the reduced/stepwise model produces comparable results to the Bayesian model for the beta coefficient estimates as seen in Table 3. Among most coefficient estimates, the differences are slightly smaller than with the full model. We also have more beta coefficients in the Bayesian model that are slightly greater than the reduced model estimates than with the full model, indicating the reduced model might have done a slightly bit more shrinkage of coefficients than the full model. Note we are comparing the full Bayesian model to the reduced OLS model with the missing predictors taken out of Table 3 for both models.

```{=latex}
\input{beta_95_CI_CR_table.tex}
\\

Table 4: Comparison of the 95\% confidence intervals (OLS) and the shortest 95\% credible regions (Bayesian) of the Beta coefficient parameters estimated using the different frameworks.
```
Table 4 above compares the 95% confidence intervals of the parameters under the ordinary least squares framework (with both the full and reduced models) and the shortest 95% credible regions of the parameters under the Bayesian framework. From looking at the table, the bounds of the credible regions are not much different compared to the bounds of the confidence intervals. We can also see that for the predictors $X_3$ (INDUS) and $X_6$ (AGE), the 95% confidence intervals and credible regions for the parameters both contain 0. This means that under both frameworks, we can conclude that the predictors $X_3$ and $X_6$ are not statistically significant to the model. Overall, the two methods match pretty closely in this regard.

```{r}
#Get the MSE version that adjusts for the number of parameters in the model to estimate sigma^2 
#(full model)
training_MSE_adj <- (training_MSE_full*nrow(train_set_dataset))/
                    (nrow(train_set_dataset)-length(coef(full_model)))
cat("Full OLS Model sigma^2 best estimate:", training_MSE_adj)

#Get the MSE version that adjusts for the number of parameters in the model to estimate sigma^2 
#(reduced model)
training_MSE_adj <- (training_MSE_reduced*nrow(train_set_dataset))/
                    (nrow(train_set_dataset)-length(coef(stepwise_model)))
cat("Reduced OLS Model sigma^2 best estimate:", training_MSE_adj)
```

```{python}
#Plot of prior for sigma^2
sigma2 = 1 / np.random.gamma(shape=alpha_post, scale=1/r_post, size=sampling_size)
mode = r_post/(alpha_post+1)
mean = r_post/(alpha_post-1)
variance = (r_post**2)/((alpha_post-1)**2 *(alpha_post-2))
print(f"Bayesian Posterior Statistics for sigma^2 \n MAP: {mode}, Mean: {mean}, Variance: {variance}")
```

We now compare the variance of the error terms under the ordinary least squares and the Bayesian framework. Under the ordinary least squares framework, the estimate for the variance is the mean squared error, or the mean sum of squares residuals. We obtain an estimate of 22.627 (the training MSE) for the full OLS model and 22.541 for the reduced OLS model. Under the Bayesian framework, we get that the estimate for the variance under the squared error loss is 21.736 and the MAP (Maximum a Posteriori) estimate is 21.462 (we can see that the variance for the posterior distribution for $\sigma^2$ is also small in the Bayesian framework, indicating confidence with the values around where the distribution is concentrated at). This shows that the estimate for the variance of the error terms of the median house prices is similar under both the ordinary least squares and the Bayesian framework.

```{=latex}
\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
& Full OLS Model & Reduced OLS Model & Bayesian Model \\ \hline
Training MSE & 21.666 & 21.730 & 21.679 \\
Training MSE (accounting for \#parameters) & 22.627 & 22.541 & 22.641 \\
Test MSE & 24.705 & 24.527 & 24.866 \\ \hline
\end{tabular}
\end{table}

Table 5: Comparison of the test and training MSE between models
```
Lastly, we can see that the training MSE accounting and not accounting for #parameters both are all super similar, as well as the MSE on the test dataset. This indicates that all methods perform similarly in fitting the median home prices. The small difference demonstrates the validity of the Bayesian method compared to the more common frequentist method, allowing us to further explore using the Bayesian framework for fitting linear models.

# Conclusion/Future Steps

All in all, we explored the differences between Ordinary Least Squares Regression (frequentist method) and the Bayesian method of settings priors to find posterior distributions for the model parameters. We analyzed the Boston Housing dataset using both OLS and Bayesian models. Overall, we found that the OLS and Bayesian estimates for the beta coefficients and $\sigma^2$ were similar in both frameworks. We note that utilizing different parameters for the priors in the Bayesian model could lead to vastly different estimates and affect the significance of the predictors. In the future, we could further explore in the Bayesian model how to create posterior predictive distributions to predict new observations, hypothesis testing of multiple parameters, creating joint credible regions, choosing different types of distributions as priors (such as Jeffrey's prior), and transformations of our data.

# References

Harrison, D., & Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. *Journal of Environmental Economics and Management*, 5, 81-102.

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). *An Introduction to Statistical Learning: With Applications in R* (2nd ed.). Springer.

Montgomery, D. C., Peck, E. A., & Vining, G. G. (2021). *Introduction to linear regression analysis* (6th ed.).

Wundervald, B. (2019, June). Bayesian linear regression. <https://doi.org/10.13140/RG.2.2.28385.97121>
